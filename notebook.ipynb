{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Abstractive Text Summarization using Pegasus***\n",
    "\n",
    "### Pre-training with Extracted Gap-sentences for Abstractive Summarization.\n",
    "\n",
    "It is specifically designed for the task of text summarization, particularly abstractive summarization, where the goal is to generate a concise and coherent summary that captures the main points of the input text, possibly with new sentences that were not in the original text\n",
    "_____________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Install PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Install Transformers from Hugging Face**\n",
    "\n",
    "We'll be using the Pegasus-xsum model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing and Loading the Model**\n",
    "\n",
    "We'll be bringing in two imports:\n",
    "\n",
    "- *PegasusForConditionalGeneration*: This will allow us to use the deep learning model.\n",
    "\n",
    "- *PegasusTokenizer*: This classs will allow sentences to convert to a set of tokens which we will then pass to our model `(NOTE: ensure you have the 'sentencepiece' library installed - you can do so through 'pip install sentencepiece' if using pip)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oliver\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perform Some Abstractive Summarization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's introduce some text\n",
    "\n",
    "text = \"\"\"\n",
    "Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google.[1][2] A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"[3]\n",
    "\n",
    "BERT was originally implemented in the English language at two model sizes:[1] (1) BERTBASE: 12 encoders with 12 bidirectional self-attention heads totaling 110 million parameters, and (2) BERTLARGE: 24 encoders with 16 bidirectional self-attention heads totaling 340 million parameters. Both models were pre-trained on the Toronto BookCorpus[4] (800M words) and English Wikipedia (2,500M words).\n",
    "\n",
    "BERT is an \"encoder-only\" transformer architecture.\n",
    "\n",
    "On a high level, BERT consists of three modules:\n",
    "\n",
    "embedding. This module converts an array of one-hot encoded tokens into an array of vectors representing the tokens.\n",
    "a stack of encoders. These encoders are the Transformer encoders. They perform transformations over the array of representation vectors.\n",
    "un-embedding. This module converts the final representation vectors into one-hot encoded tokens again.\n",
    "The un-embedding module is necessary for pretraining, but it is often unnecessary for downstream tasks. Instead, one would take the representation vectors output at the end of the stack of encoders, and use those as a vector representation of the text input, and train a smaller model on top of that.\n",
    "\n",
    "BERT uses WordPiece to convert each English word into an integer code. Its vocabulary has size 30,000. Any token not appearing in its vocabulary is replaced by [UNK] for \"unknown\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 7671, 37390, 93789, 37955,   116,   135, 38979,   143, 62613,   158,\n",
      "           117,   114,  1261,   861,   451,   124,   109, 22470,  3105,   108,\n",
      "          7913,   118,   203,  5110,  2757,   204,  1331,   449,   113,   109,\n",
      "           691,  1581,   107,   168,   140,  2454,   115,  1350,   931,   141,\n",
      "          2995,   134,  1058,   107, 65077, 32887, 50558,   202,  7149,  4413,\n",
      "          2629,  7111,   120,   198,   386,   114,   332,   204,   114,   232,\n",
      "           108,   110, 62613,   148,   460,   114, 20410, 13757,   115,  4284,\n",
      "          7148, 11430,   143, 72237,   158,  8026,  8742,   204,  3968,   473,\n",
      "          6185, 10850,   111,  3024,   109,   861,   107, 54151, 59740,   110,\n",
      "         62613,   140,  3273,  4440,   115,   109,  1188,  1261,   134,   228,\n",
      "           861,  2568,   151, 65077,  1100,  6806,   110, 62613, 51534,   151,\n",
      "           665, 40753,   116,   122,   665, 79050,   813,   121, 65167,  4082,\n",
      "           916,   273,  8558,   604,  5384,   108,   111,  7223,   110, 62613,\n",
      "         92478,   151,  1202, 40753,   116,   122,  1195, 79050,   813,   121,\n",
      "         65167,  4082,   916,   273, 26959,   604,  5384,   107,  2595,  1581,\n",
      "           195,  1133,   121, 14787,   124,   109,  4268,  2459, 40817,  1579,\n",
      "          4101, 60708,   143,  9671,   897,   989,   158,   111,  1188, 11099,\n",
      "           143, 39338,   897,   989,   250,   110, 62613,   117,   142,   198,\n",
      "          1227, 56636,   121,  6026,   194, 22470,  3105,   107,   651,   114,\n",
      "           281,   476,   108,   110, 62613,  3471,   113,   339,  6364,   151,\n",
      "         35727,   107,   182,  4352, 18401,   142,  3561,   113,   156,   121,\n",
      "         12928, 33041, 14426,   190,   142,  3561,   113, 28089,  5184,   109,\n",
      "         14426,   107,   114,  6968,   113, 40753,   116,   107,   507, 40753,\n",
      "           116,   127,   109, 51979, 40753,   116,   107,   322,  1798, 26595,\n",
      "           204,   109,  3561,   113,  5114, 28089,   107,  1596,   121,  4192,\n",
      "         75375,   107,   182,  4352, 18401,   109,   976,  5114, 28089,   190,\n",
      "           156,   121, 12928, 33041, 14426,   435,   107,   139,  1596,   121,\n",
      "          4192, 75375,  4352,   117,   993,   118,  1133, 18006,   108,   155,\n",
      "           126,   117,   432,  6945,   118, 18030,  2722,   107,  3054,   108,\n",
      "           156,   192,   248,   109,  5114, 28089,  2940,   134,   109,   370,\n",
      "           113,   109,  6968,   113, 40753,   116,   108,   111,   207,   274,\n",
      "           130,   114,  9359,  5114,   113,   109,  1352,  3196,   108,   111,\n",
      "          1976,   114,  1934,   861,   124,   349,   113,   120,   107,   110,\n",
      "         62613,  1481,  3985, 47096,   112,  4689,   276,  1188,  1172,   190,\n",
      "           142, 26770,   929,   107,  3096, 10987,   148,   628, 14802,   107,\n",
      "          2533, 10844,   146,  8741,   115,   203, 10987,   117,  3530,   141,\n",
      "          1126, 88904,  1100,   118,   198, 68247,  2302,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "# Next we'll next to convert this text into its tokenized/number representation\n",
    "\n",
    "tokens = tokenizer(text, truncation=True, padding='longest', return_tensors='pt') # 'truncation=True' will shorten our text as there are limits as to how much we can pass into the model and we want to return pytorch tensors with 'return_tensors='pt'\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  7671, 37390, 37955,   116,   135, 38979,   143, 62613,   158,\n",
       "           117,   114,  1261,   861,   451,   124,   109, 22470,  3105,   108,\n",
       "          7913,   118,   203,  5110,  2757,   204,  1331,   449,   113,   109,\n",
       "           691,  1581,   107,     1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try summarize the text now\n",
    "\n",
    "summary = model.generate(**tokens) # '**tokens' essentially unpacks everything seen above\n",
    "\n",
    "# Summary in tokens\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bidirectional Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above tensor doesnt actually provide us with much meaning however, we can again utilize our tokenizer to decode the above and extract our summarization. \n",
    "\n",
    "# what we need is actully nested so let's grab the first instance of our result\n",
    "\n",
    "tokenizer.decode(summary[0], skip_special_tokens=True) # we can skip special tokens to remove special tokens that appear at the start and end like '<pad>' or '</s>'\n",
    "\n",
    "\n",
    "# We can somewhat validate our results by copying the output below and doing a search on this page: \"https://en.wikipedia.org/wiki/BERT_(language_model)\" where the text above was extracted from.\n",
    "# You'll see that there are 0 results which means the output is 100% original and it indeed did perform abstraction rather than extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Let's do another piece of text**\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Machine_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"\"\"\n",
    "The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[9][10] The synonym self-teaching computers was also used in this time period.[11][12]\n",
    "\n",
    "Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[13] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[14] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[13] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[13]\n",
    "\n",
    "By the early 1960s an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to re-evaluate incorrect decisions.[15] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[16] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[17] In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[18]\n",
    "\n",
    "Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\"[19] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".[20]\n",
    "\n",
    "Modern-day machine learning has two objectives. One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  139,  1286,  1157,   761,   140, 27775,   115, 22570,   141,  9429,\n",
       "         10070,   108,   142,  7313,  2307,   111, 12649,   115,   109,   764,\n",
       "           113,   958,  3982,   111,  4958,  3941,   107,  4101,  2507, 32887,\n",
       "          2449,  1100,   139, 47248,   813,   121, 46490,  4328,   140,   163,\n",
       "           263,   115,   136,   166,   908,   107,  4101,  4363, 32887,  3602,\n",
       "          1100,  2113,   109,  9441,  1157,   761,   861,   140,  2454,   115,\n",
       "           109,  7765,   116,   173,  9429, 10070, 11553,   114,   431,   120,\n",
       "          7123,   109,  2269,  1012,   115, 67338,   118,   276,   477,   108,\n",
       "           109,   689,   113,  1157,   761,  4663,   247,   112,  2701,   113,\n",
       "           883,  2524,   111,  1441,   112,   692,   883,  7842,  1994,   107,\n",
       "         65077, 59740,   222, 20322,   108,  3066, 17518,  5502,   285, 12750,\n",
       "          1299,   109,   410,   139,  7235,   113, 20786,   108,   115,   162,\n",
       "           178,  2454,   114,  9637, 14849,  1557,  3373,   141,   878,  6143,\n",
       "           790,  8165,  2361,   107, 65077, 60708,   285, 12750,   131,   116,\n",
       "           861,   113, 21708, 13543,   122,   156,   372,   323,   114, 30203,\n",
       "           118,   199,  5344,   116,   111,  1157,   761,  8970,   201,   365,\n",
       "         11406,   108,   132,  4958, 21708,   263,   141,  4328,   112,  3400,\n",
       "           335,   107, 65077, 59740,  2428,  2995,   170,   133,  4525,   883,\n",
       "          7842,   747,  5674,   112,   109,   946,  1157,   761,  2150,   130,\n",
       "           210,   108,   330,  6936,  3262, 10704, 60177,   111,  9339, 85876,\n",
       "         16692,   108,   170,  2962,   109,   616, 11963,  1581,   113, 14849,\n",
       "          3296,   112,   331,   164,   122,  8970,   120,  3691,   883,   666,\n",
       "          1994,   107, 65077, 59740,  1060,   109,   616,  6939,   116,   142,\n",
       "          7707,   198, 13049,  1157,   194,   122, 28237,  4481,  1959,   108,\n",
       "           568,  9826, 13368,   108,   196,   174,  1184,   141, 80327,  1555,\n",
       "           112,  5935, 53495,  6466,   108,   860, 95203,   116,   108,   111,\n",
       "          3442,  2890,   303, 50158, 19189,   761,   107,   168,   140, 15345,\n",
       "           445,   198, 14787,   194,   141,   114,   883,  5251,   191, 36787,\n",
       "           112,  3326,  2890,   111,  2852,   122,   114,   198,  2967,  1313,\n",
       "           194,  1475,   112,  1007,   126,   112,   920,   121, 39737,  9206,\n",
       "          1993,   107,  4101,  4262,  1100,   202,  4340,   410,   124,   473,\n",
       "           190,  1157,   761,   333,   109,  6939,   116,   140, 77224,   131,\n",
       "           116,   410,   124,  4473, 17607,   108,  3297,  2320,   122,  1157,\n",
       "           761,   118,  2293, 10526,   107,  4101,  5357,  1100, 13123,   985,\n",
       "           112,  2293,  3771,  2059,   190,   109,  6789,   116,   108,   130,\n",
       "          2540,   141, 88054,   111,  9322,   115, 39741,  4101,  6113,  1100,\n",
       "           222, 17298,   114,   731,   140,   634,   124,   303,  1703,  2175,\n",
       "           167,   120,   142,  4958, 14849,   952, 23153,   112,  3326,  1466,\n",
       "          1918, 32369,  3439,   108,   377, 15865,   108,   111,   384,   548,\n",
       "          7205,   158,   135,   114,   958,  6577,   107,  4101,  4876,  1100,\n",
       "          3227,   627,   107,  9653,   735,   114,  3447,  9229,   108,   154,\n",
       "          3722,  3955,   113,   109,  8970,  4525,   115,   109,  1157,   761,\n",
       "           764,   151,   198,   251,   958,   431,   117,   243,   112,   543,\n",
       "           135,   306,   699,   122,  2132,   112,   181,   755,   113,  2722,\n",
       "           781,   111,   637,  2488,   881,   175,   203,   637,   134,  2722,\n",
       "           115,   781,   108,   130,  5844,   141,   881,   108,  7997,   122,\n",
       "           306,   699,   496, 65077, 76207,   182,  3955,   113,   109,  2722,\n",
       "           115,   162,  1157,   761,   117,  2993,   504,   114, 16521,  4594,\n",
       "          3955,   880,   197, 10496,   109,   764,   115,  7842,  1130,   107,\n",
       "           182,  4083,  7973, 61561,   131,   116,  3993,   115,   169,   800,\n",
       "           198, 26010, 34752, 12361,   111,  8630,   194,   108,   115,   162,\n",
       "           109,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_2 = tokenizer(text_2, truncation=True, padding='longest', return_tensors='pt')\n",
    "\n",
    "tokens_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 3838,  761,  117,  109,  692,  113,  199, 4328,  543,  135,  306,\n",
       "          107,    1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_2 = model.generate(**tokens_2)\n",
    "\n",
    "summary_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is the study of how computers learn from experience.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(summary_2[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Let's do recent news article on Nvidia**\n",
    "\n",
    "Source: https://finance.yahoo.com/news/nvidia-stock-buy-143000315.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAB7TYDBbY4owo_Iy0uwhI8_mBjSfZwwiQQguLUxyJ1dU6ZNhNGsLWLJpUqv4oQFLpsbVdNcXOCZRHK-OepxiB5c_yr_D6f6fUWNkaMk1r_rQHbYTU1edHQm9584mDmywrMVxI0Ff_z528hifW-1jESyDeRfOXxqzSKG8m1k_gy5S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = \"\"\"\n",
    "Nvidia (NASDAQ: NVDA) won over Wall Street last year, illustrated by its more than 280% stock growth since March 2023. The company's years of dominance in graphics processing units (GPUs) perfectly positioned it to profit significantly from a boom in artificial intelligence (AI) as demand for the chips skyrocketed. As a result, Nvidia's quarterly revenue and free cash flow are up 207% and 430%, respectively, in the last 12 months.\n",
    "\n",
    "The company's meteoric rise has some analysts questioning whether the company has much more to offer investors in 2024. However, trends in the chip market indicate Nvidia will have little problem retaining its leading market share in AI GPUs, despite new offerings from Advanced Micro Devices and Intel.\n",
    "\n",
    "Meanwhile, the AI market is nowhere near hitting its ceiling. It's projected to expand at a compound annual growth rate of 37% until at least 2030. The sector's potential indicates GPU demand is likely to continue rising for the foreseeable future, with Nvidia well-equipped to continue enjoying significant gains from AI.\n",
    "\n",
    "Here's why Nvidia remains an attractive buy in March.\n",
    "\n",
    "Nvidia will likely retain its AI dominance despite rising competition\n",
    "Nvidia's success in the AI chip market has led to countless tech companies announcing ventures into the industry. Leading chipmakers AMD and Intel plan to begin shipping new GPUs soon in an attempt to challenge Nvidia's market share. Meanwhile, companies new to the sector are also joining in, as Amazon and Microsoft announced new AI chips last year.\n",
    "\n",
    "However, market trends suggest Nvidia's supremacy will be challenging for competitors to overcome. The company has held an over 80% market share in desktop GPUs for years, despite AMD's and Intel's presence in the sector.\n",
    "\n",
    "Intel only entered the industry last year, while AMD's history in desktop GPUs spans decades. Still, AMD's GPUs only account for about 10% of the market.\n",
    "\n",
    "A similar situation has occurred in another area of the chip market. Intel was a king in central processing units (CPUs) for years, with an 82% market share at the start of 2017 when AMD landed on the scene with its Ryzen line of CPUs. AMD has managed to steal a significant share from Intel since then. However, Intel is still responsible for most of the CPU market; its share is above 60% and AMD's is at 36%.\n",
    "\n",
    "Nvidia's estimated 80% to 95% market share in AI GPUs could falter slightly as competition heats up. However, history indicates the company will retain its overall lead and continue to see major gains from AI for years.\n",
    "\n",
    "Projections show Nvidia's stock should continue beating the S&P 500\n",
    "Nvidia has stunned Wall Street over the last year, posting multiple quarters of record earnings. In the fourth quarter of 2024 (ended in January), the company's revenue increased by 265% year over year to $22 billion. Meanwhile, operating income jumped 983% to nearly $14 billion. The monster growth was primarily from a 409% increase in data center revenue, reflecting increased chip sales.\n",
    "\n",
    "While a spike in AI GPU sales is mainly responsible for Nvidia's stellar financial growth, the chipmaker is also profiting from an improving PC market. Spikes in inflation prompted steep declines in PC sales, with shipments dipping 16% in 2022 and continuing to fall for most of 2023. However, recent reports indicate the market is finally showing signs of recovery.\n",
    "\n",
    "According to Gartner, PC shipments popped 0.3% in Q4 2023, marking the first such increase in over a year. Market improvements have been reflected in Nvidia's sales, with its PC-centered gaming segment reporting an 81% rise in revenue in Q3 2024 (which ended October 2023).\n",
    "\n",
    "A leading role in AI and a recovering PC market suggests Nvidia has a strong outlook in the coming years. Earnings-per-share (EPS) estimates seem to support this.\n",
    "\n",
    "NVDA EPS Estimates for 2 Fiscal Years Ahead Chart\n",
    "NVDA EPS Estimates for 2 Fiscal Years Ahead Chart\n",
    "The above chart shows Nvidia's earnings could hit $34 per share by fiscal 2026. Multiplying that figure by its forward price-to-earnings ratio of 38 yields a stock price of $1,292.\n",
    "\n",
    "Considering the company's current position, that projection would see Nvidia's stock rise 40% over the next two years. The company may not replicate last year's growth but would still beat the S&P 500's 22% growth since 2022.\n",
    "\n",
    "As a result, Nvidia still has much to offer new investors and is an exciting buy right now.\n",
    "\n",
    "Should you invest $1,000 in Nvidia right now?\n",
    "\n",
    "Before you buy stock in Nvidia, consider this:\n",
    "\n",
    "The Motley Fool Stock Advisor analyst team just identified what they believe are the 10 best stocks for investors to buy now… and Nvidia wasn’t one of them. The 10 stocks that made the cut could produce monster returns in the coming years.\n",
    "\n",
    "Stock Advisor provides investors with an easy-to-follow blueprint for success, including guidance on building a portfolio, regular updates from analysts, and two new stock picks each month. The Stock Advisor service has more than tripled the return of S&P 500 since 2002*.\n",
    "\n",
    "See the 10 stocks\n",
    "\n",
    "*Stock Advisor returns as of March 11, 2024\n",
    "\n",
    "John Mackey, former CEO of Whole Foods Market, an Amazon subsidiary, is a member of The Motley Fool’s board of directors. Dani Cook has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends Advanced Micro Devices, Amazon, Microsoft, and Nvidia. The Motley Fool recommends Gartner and Intel and recommends the following options: long January 2023 $57.50 calls on Intel, long January 2025 $45 calls on Intel, long January 2026 $395 calls on Microsoft, short January 2026 $405 calls on Microsoft, and short May 2024 $47 calls on Intel. The Motley Fool has a disclosure policy.\n",
    "\n",
    "Is Nvidia Stock a Buy? was originally published by The Motley Fool\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[30859,   143, 18482,   151, 15047, 15690,   158,   576,   204,  2948,\n",
       "          1411,   289,   232,   108,  9789,   141,   203,   154,   197,   280,\n",
       "         34939,  1279,   874,   381,  1051, 30249,   107,   139,   301,   131,\n",
       "           116,   231,   113, 19224,   115,  3647,  2196,  2022,   143, 58454,\n",
       "           116,   158,  2475,  8523,   126,   112,  3508,  2838,   135,   114,\n",
       "          9862,   115,  4958,  3941,   143, 13901,   158,   130,  1806,   118,\n",
       "           109,  5162, 52082,   107,   398,   114,   711,   108, 30859,   131,\n",
       "           116, 10337,  2563,   111,   294,  1325,  1971,   127,   164,   599,\n",
       "         22436,   111,   384, 41074,   108,  4802,   108,   115,   109,   289,\n",
       "           665,   590,   107,   139,   301,   131,   116, 77039,  2423,   148,\n",
       "           181,  8067, 12817,   682,   109,   301,   148,   249,   154,   112,\n",
       "           369,  2714,   115, 34074,   107,   611,   108,  2994,   115,   109,\n",
       "          6263,   407,  4298, 30859,   138,   133,   332,   575, 11210,   203,\n",
       "           964,   407,   537,   115,  5344, 42296,   108,  2409,   177,  5942,\n",
       "           135,  5435,  6586, 17641,   111,  7265,   107,  8199,   108,   109,\n",
       "          5344,   407,   117,  9011,   828,  5876,   203,  3307,   107,   168,\n",
       "           131,   116,  9132,   112,  3086,   134,   114,  8191,  1450,   874,\n",
       "           872,   113, 41752,   430,   134,   583, 41918,   139,  1827,   131,\n",
       "           116,   866,  5767, 15405,  1806,   117,   770,   112,   801,  4220,\n",
       "           118,   109, 26083,   533,   108,   122, 30859,   210,   121, 17748,\n",
       "           112,   801,  3462,  1225,  6602,   135,  5344,   107,  1063,   131,\n",
       "           116,   447, 30859,  2085,   142,  3269,   631,   115,  1051,   107,\n",
       "         30859,   138,   770,  5515,   203,  5344, 19224,  2409,  4220,  1702,\n",
       "         30859,   131,   116,   924,   115,   109,  5344,  6263,   407,   148,\n",
       "          1358,   112,  6150,  3278,   524, 13501, 15488,   190,   109,   503,\n",
       "           107, 15942,  6263, 10005, 14260,   111,  7265,   511,   112,  1213,\n",
       "          2129,   177, 42296,   783,   115,   142,  2353,   112,  1459, 30859,\n",
       "           131,   116,   407,   537,   107,  8199,   108,   524,   177,   112,\n",
       "           109,  1827,   127,   163,  3569,   115,   108,   130,  2107,   111,\n",
       "          2167,  1487,   177,  5344,  5162,   289,   232,   107,   611,   108,\n",
       "           407,  2994,  2298, 30859,   131,   116, 38153,   138,   129,  2782,\n",
       "           118,  4929,   112,  4798,   107,   139,   301,   148,   886,   142,\n",
       "           204,  8296,   407,   537,   115,  4305, 42296,   118,   231,   108,\n",
       "          2409, 14260,   131,   116,   111,  7265,   131,   116,  2210,   115,\n",
       "           109,  1827,   107,  7265,   209,  3295,   109,   503,   289,   232,\n",
       "           108,   277, 14260,   131,   116,   689,   115,  4305, 42296, 14916,\n",
       "          2701,   107,  4587,   108, 14260,   131,   116, 42296,   209,   728,\n",
       "           118,   160,  5029,   113,   109,   407,   107,   202,   984,  1288,\n",
       "           148,  4606,   115,   372,   345,   113,   109,  6263,   407,   107,\n",
       "          7265,   140,   114,  4138,   115,  2056,  2196,  2022,   143, 71150,\n",
       "           116,   158,   118,   231,   108,   122,   142, 56824,   407,   537,\n",
       "           134,   109,   388,   113,  1326,   173, 14260,  8809,   124,   109,\n",
       "          2166,   122,   203, 70631,   540,   113, 49405,   107, 14260,   148,\n",
       "          2079,   112,  9129,   114,  1225,   537,   135,  7265,   381,   237,\n",
       "           107,   611,   108,  7265,   117,   309,  1470,   118,   205,   113,\n",
       "           109,  9307,   407,   206,   203,   537,   117,   607, 10713,   111,\n",
       "         14260,   131,   116,   117,   134, 43693,   107, 30859,   131,   116,\n",
       "          3627,  8296,   112, 13556,   407,   537,   115,  5344, 42296,   256,\n",
       "         54347,  2237,   130,  1702, 41593,   164,   107,   611,   108,   689,\n",
       "          5767,   109,   301,   138,  5515,   203,  1380,   756,   111,   801,\n",
       "           112,   236,   698,  6602,   135,  5344,   118,   231,   107, 54682,\n",
       "           116,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_3 = tokenizer(text_3, truncation=True, padding='longest', return_tensors='pt')\n",
    "\n",
    "tokens_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   240,   119,   131,   261,   174,   124,   109,  6662,   118,\n",
       "           114,  3278,  1279,   112,   631,   115,  1051,   108,   119,   382,\n",
       "           245,   112,  1037, 30859,   107, 30859,   138,   770,  5515,   203,\n",
       "          5344, 19224,  2409,  4220,  1702, 30859,   131,   116,   924,   115,\n",
       "           109,  5344,  6263,   407,   148,  1358,   112,  6150,  3278,   524,\n",
       "         13501, 15488,   190,   109,   503,   107,     1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_3 = model.generate(**tokens_3)\n",
    "\n",
    "summary_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If you've been on the hunt for a tech stock to buy in March, you might want to consider Nvidia. Nvidia will likely retain its AI dominance despite rising competition Nvidia's success in the AI chip market has led to countless tech companies announcing ventures into the industry.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(summary_3[0], skip_special_tokens=True)\n",
    "\n",
    "# And again, the below cannot be found in the news article which signals a success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
